{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/teyoon1105/deeplearning/blob/main/food_classification_train_valid.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### kaggle.json 파일을 불러와 kaggle key값 가져오기"
      ],
      "metadata": {
        "id": "p6HizpM9yNx4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FS8aMYkK1uRB"
      },
      "outputs": [],
      "source": [
        "KAGGLE_UPLOAD = False\n",
        "\n",
        "if KAGGLE_UPLOAD:\n",
        "    # /content/ 폴더에 kaggle.json을 업로드 (매번 colab노트북 생성시 반복)\n",
        "    from google.colab import files\n",
        "    files.upload()\n",
        "else:\n",
        "    kaggle_username = 'teyoon'\n",
        "    from google.colab import userdata\n",
        "    kaggle_key = userdata.get(kaggle_username)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "def create_kaggle_json(username, key, file_path):\n",
        "    # 데이터 생성\n",
        "    kaggle_data = {\n",
        "        \"username\": username,\n",
        "        \"key\": key\n",
        "    }\n",
        "\n",
        "    # JSON 파일로 저장\n",
        "    with open(file_path, 'w') as json_file:\n",
        "        json.dump(kaggle_data, json_file)\n",
        "\n",
        "    print(f\"kaggle.json 파일이 '{os.path.abspath(file_path)}' 경로에 생성되었습니다.\")\n",
        "\n",
        "\n",
        "\n",
        "# kaggle.json 파일 생성 함수 호출\n",
        "if KAGGLE_UPLOAD==False:\n",
        "    # 사용자 정보 입력\n",
        "    key = kaggle_key  # 여기에 key값을 입력하세요\n",
        "    filename = 'kaggle.json'\n",
        "    create_kaggle_json(kaggle_username, key, filename)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLXOocO52Tcx",
        "outputId": "581568f3-5358-4c17-c945-a8de7588c402"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kaggle.json 파일이 '/content/kaggle.json' 경로에 생성되었습니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### kaggle data 사용하기 위해 root 디렉토리 아래 .kaggle 파일 복사"
      ],
      "metadata": {
        "id": "fHDWlwu3yTxb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "os.makedirs('/root/.kaggle', exist_ok=True)\n",
        "path = os.getcwd()\n",
        "\n",
        "if not os.path.exists(os.path.join('/root/.kaggle', 'kaggle.json')):\n",
        "    src = os.path.join(path, 'kaggle.json')\n",
        "    dst = os.path.join('/root/.kaggle', 'kaggle.json')\n",
        "    shutil.copy(src, dst)"
      ],
      "metadata": {
        "id": "gSC84SAY2iSJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### kaggle data 다운로드"
      ],
      "metadata": {
        "id": "g4YM1IGq3wJt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod 700 /root/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d harishkumardatalab/food-image-classification-dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-YVNqqJ2hX0",
        "outputId": "139cb270-cae1-4311-cd22-c5f80be9eca9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/harishkumardatalab/food-image-classification-dataset\n",
            "License(s): CC0-1.0\n",
            "Downloading food-image-classification-dataset.zip to /content\n",
            "100% 1.68G/1.68G [01:20<00:00, 21.5MB/s]\n",
            "100% 1.68G/1.68G [01:20<00:00, 22.4MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 다운된 kaggle zip 파일 unzip"
      ],
      "metadata": {
        "id": "61A54EVa33jn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -qq food-image-classification-dataset.zip -d /content/food_data"
      ],
      "metadata": {
        "id": "oFdJIg522E3Z"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 압축해제한 zip 파일의 이름을 가독성 좋게 food_data로 변경"
      ],
      "metadata": {
        "id": "L_IvfFjtUQ4q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "folder_name = \"/content/food_data/Food Classification dataset\"\n",
        "new_folder_name = \"/content/food_data/train_dataset\"\n",
        "\n",
        "# 폴더 이름 변경\n",
        "os.rename(folder_name, new_folder_name)\n",
        "\n",
        "print(f'폴더 이름이 {new_folder_name}으로 변경되었습니다.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "id": "qOJHgLQL5AR7",
        "outputId": "967bf280-e770-4939-b9b0-2336df941b28"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/food_data/Food Classification dataset' -> '/content/food_data/train_dataset'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-ee4faaaa9e44>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# 폴더 이름 변경\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_folder_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'폴더 이름이 {new_folder_name}으로 변경되었습니다.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/food_data/Food Classification dataset' -> '/content/food_data/train_dataset'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### train, valid, test 데이터의 경로, 디렉토리 설정"
      ],
      "metadata": {
        "id": "gjouDj6fycPu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data path를 만들고\n",
        "# valid path 만들고 해당 폴더에 클래스 갯수에 맞게 폴더 생성\n",
        "dataPath = '/content/food_data/'\n",
        "trainPath = os.path.join(dataPath, 'train_dataset/')\n",
        "validPath = '/content/food_data/valid_dataset/'\n",
        "os.makedirs(validPath, exist_ok=True)\n",
        "testPath = '/content/food_data/test_dataset/'\n",
        "os.makedirs(testPath, exist_ok=True)"
      ],
      "metadata": {
        "id": "UkHSSP3_QXDN"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 색 채널 변경 코드"
      ],
      "metadata": {
        "id": "gfqJvFl_0NdR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "def convert_image_to_rgb(image_path):\n",
        "    # Pillow로 이미지 열기\n",
        "    img_pil = Image.open(image_path)\n",
        "\n",
        "    if img_pil.mode != 'RGB':\n",
        "        # Pillow 이미지 모드에 따라 처리\n",
        "        if img_pil.mode == 'P':  # P 모드 (팔레트 모드)\n",
        "            print(\"Converting P mode to RGB...\")\n",
        "            img_pil = img_pil.convert('RGB')\n",
        "\n",
        "        elif img_pil.mode == 'RGBA':  # RGBA 모드 (알파 채널 포함)\n",
        "            print(\"Converting RGBA to RGB...\")\n",
        "            img_pil = img_pil.convert('RGB')\n",
        "\n",
        "        elif img_pil.mode == 'CMYK':  # CMYK 모드\n",
        "            print(\"Converting CMYK to RGB...\")\n",
        "            img_pil = img_pil.convert('RGB')\n",
        "\n",
        "\n",
        "        img_pil.save(image_path)\n",
        "        print(f\"Image saved at: {image_path}\")\n",
        "\n",
        "food_list = os.listdir(trainPath)\n",
        "for food in food_list:\n",
        "    img_path = os.path.join(trainPath, food)\n",
        "    for filename in os.listdir(img_path):\n",
        "        image_path = os.path.join(img_path, filename)\n",
        "        convert_image_to_rgb(image_path)\n",
        "        image = Image.open(os.path.join(img_path, filename))\n",
        "        if image.mode != 'RGB':\n",
        "            print(f'RGB로 전환되지 않았습니다 : {image.mode}')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "I75RXtmT0Jag"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### train data 확인"
      ],
      "metadata": {
        "id": "3X2nV2Gzue8O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "food_list = os.listdir(trainPath)\n",
        "print(len(food_list))\n",
        "for food in food_list:\n",
        "    path = os.path.join(trainPath, food)\n",
        "    print(food, len(os.listdir(path)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VmCVXIa2uegs",
        "outputId": "f52a5694-4758-4c54-85e5-c350eb18adb3",
        "collapsed": true
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34\n",
            "kaathi_rolls 168\n",
            "ice_cream 600\n",
            "Sandwich 900\n",
            "Donut 900\n",
            "chole_bhature 226\n",
            "chai 207\n",
            "apple_pie 600\n",
            "cheesecake 600\n",
            "fried_rice 210\n",
            "momos 195\n",
            "pizza 165\n",
            "idli 182\n",
            "Crispy Chicken 900\n",
            "samosa 147\n",
            "paani_puri 87\n",
            "kulfi 129\n",
            "pav_bhaji 183\n",
            "burger 199\n",
            "omelette 600\n",
            "Taco 900\n",
            "jalebi 174\n",
            "Fries 900\n",
            "dhokla 147\n",
            "dal_makhani 177\n",
            "Taquito 900\n",
            "Hot Dog 930\n",
            "Baked Potato 900\n",
            "butter_naan 185\n",
            "sushi 600\n",
            "pakode 162\n",
            "chicken_curry 600\n",
            "chapati 197\n",
            "masala_dosa 165\n",
            "kadai_paneer 201\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(os.listdir(trainPath))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvS47bkZRMGX",
        "outputId": "569097ea-d7b9-4a91-d2b1-8c78cce57ab7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['chicken_curry', 'chapati', 'dhokla', 'fried_rice', 'chai', 'Crispy Chicken', 'Fries', 'burger', 'omelette', 'pizza', 'kaathi_rolls', 'kulfi', 'paani_puri', 'Sandwich', 'dal_makhani', 'Baked Potato', 'kadai_paneer', 'chole_bhature', 'cheesecake', 'momos', 'pav_bhaji', 'masala_dosa', 'Taquito', 'sushi', 'samosa', 'Taco', 'Hot Dog', 'idli', 'butter_naan', 'Donut', 'apple_pie', 'pakode', 'ice_cream', 'jalebi']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### valid data에 train 데이터와 같은 폴더 생성"
      ],
      "metadata": {
        "id": "cipYv3payhxJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 해당 경로에 train data의 폴더 갯수에 맞게 클래스 별로 하위 폴더 생성\n",
        "valid_list = os.listdir(trainPath)\n",
        "for folder in valid_list:\n",
        "    os.makedirs(os.path.join(validPath, folder), exist_ok=True)"
      ],
      "metadata": {
        "id": "h0DfFXnoQ0ki"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### valid data에 train 데이터와 같은 폴더 생성"
      ],
      "metadata": {
        "id": "dpflOmhzysrn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_list = os.listdir(trainPath)\n",
        "for folder in test_list:\n",
        "    os.makedirs(os.path.join(testPath, folder), exist_ok=True)"
      ],
      "metadata": {
        "id": "2PBbxRGVGDaN"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### train data의 0.15를 test data로 사용"
      ],
      "metadata": {
        "id": "TnNMpWnwClSg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "test_ratio = 0.2\n",
        "\n",
        "subfolder = os.listdir(trainPath)\n",
        "for folder in subfolder:\n",
        "    train_subPath = os.path.join(trainPath, folder)\n",
        "    test_subPath = os.path.join(testPath, folder)\n",
        "\n",
        "    file_list = os.listdir(train_subPath)\n",
        "    random.shuffle(file_list)\n",
        "\n",
        "    random_files = random.sample(file_list, int(len(file_list)*test_ratio))\n",
        "    for files in random_files:\n",
        "        oldPath = os.path.join(train_subPath, files)\n",
        "        newPath = os.path.join(test_subPath, files)\n",
        "        if os.path.isfile(oldPath):\n",
        "            shutil.move(oldPath, newPath)\n",
        "\n",
        "    print(f'{folder}파일이 train_data에서 test_data로 복사되었습니다.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8UMKYxfCk8n",
        "outputId": "55875f93-473c-4211-d580-2d8010ac878b",
        "collapsed": true
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kaathi_rolls파일이 train_data에서 test_data로 복사되었습니다.\n",
            "ice_cream파일이 train_data에서 test_data로 복사되었습니다.\n",
            "Sandwich파일이 train_data에서 test_data로 복사되었습니다.\n",
            "Donut파일이 train_data에서 test_data로 복사되었습니다.\n",
            "chole_bhature파일이 train_data에서 test_data로 복사되었습니다.\n",
            "chai파일이 train_data에서 test_data로 복사되었습니다.\n",
            "apple_pie파일이 train_data에서 test_data로 복사되었습니다.\n",
            "cheesecake파일이 train_data에서 test_data로 복사되었습니다.\n",
            "fried_rice파일이 train_data에서 test_data로 복사되었습니다.\n",
            "momos파일이 train_data에서 test_data로 복사되었습니다.\n",
            "pizza파일이 train_data에서 test_data로 복사되었습니다.\n",
            "idli파일이 train_data에서 test_data로 복사되었습니다.\n",
            "Crispy Chicken파일이 train_data에서 test_data로 복사되었습니다.\n",
            "samosa파일이 train_data에서 test_data로 복사되었습니다.\n",
            "paani_puri파일이 train_data에서 test_data로 복사되었습니다.\n",
            "kulfi파일이 train_data에서 test_data로 복사되었습니다.\n",
            "pav_bhaji파일이 train_data에서 test_data로 복사되었습니다.\n",
            "burger파일이 train_data에서 test_data로 복사되었습니다.\n",
            "omelette파일이 train_data에서 test_data로 복사되었습니다.\n",
            "Taco파일이 train_data에서 test_data로 복사되었습니다.\n",
            "jalebi파일이 train_data에서 test_data로 복사되었습니다.\n",
            "Fries파일이 train_data에서 test_data로 복사되었습니다.\n",
            "dhokla파일이 train_data에서 test_data로 복사되었습니다.\n",
            "dal_makhani파일이 train_data에서 test_data로 복사되었습니다.\n",
            "Taquito파일이 train_data에서 test_data로 복사되었습니다.\n",
            "Hot Dog파일이 train_data에서 test_data로 복사되었습니다.\n",
            "Baked Potato파일이 train_data에서 test_data로 복사되었습니다.\n",
            "butter_naan파일이 train_data에서 test_data로 복사되었습니다.\n",
            "sushi파일이 train_data에서 test_data로 복사되었습니다.\n",
            "pakode파일이 train_data에서 test_data로 복사되었습니다.\n",
            "chicken_curry파일이 train_data에서 test_data로 복사되었습니다.\n",
            "chapati파일이 train_data에서 test_data로 복사되었습니다.\n",
            "masala_dosa파일이 train_data에서 test_data로 복사되었습니다.\n",
            "kadai_paneer파일이 train_data에서 test_data로 복사되었습니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### train 데이터의 0.15를  valid로 이동"
      ],
      "metadata": {
        "id": "tA0B9G89UE_0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "valid_ratio = 0.25\n",
        "\n",
        "\n",
        "subfolder = os.listdir(trainPath)\n",
        "for folder in subfolder:\n",
        "    train_subPath = os.path.join(trainPath, folder)\n",
        "    valid_subPath = os.path.join(validPath, folder)\n",
        "\n",
        "    file_list = os.listdir(train_subPath)\n",
        "    random.shuffle(file_list)\n",
        "\n",
        "    random_files = random.sample(file_list, int(len(file_list)*valid_ratio))\n",
        "    for files in random_files:\n",
        "        oldPath = os.path.join(train_subPath, files)\n",
        "        newPath = os.path.join(valid_subPath, files)\n",
        "        if os.path.isfile(oldPath):\n",
        "            shutil.move(oldPath, newPath)\n",
        "\n",
        "    print(f'{folder}파일이 train_data에서 valid_data로 복사되었습니다.')\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-VPDEJoRRpjD",
        "outputId": "14375ff4-70d7-47db-873c-361638bab285",
        "collapsed": true
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kaathi_rolls파일이 train_data에서 valid_data로 복사되었습니다.\n",
            "ice_cream파일이 train_data에서 valid_data로 복사되었습니다.\n",
            "Sandwich파일이 train_data에서 valid_data로 복사되었습니다.\n",
            "Donut파일이 train_data에서 valid_data로 복사되었습니다.\n",
            "chole_bhature파일이 train_data에서 valid_data로 복사되었습니다.\n",
            "chai파일이 train_data에서 valid_data로 복사되었습니다.\n",
            "apple_pie파일이 train_data에서 valid_data로 복사되었습니다.\n",
            "cheesecake파일이 train_data에서 valid_data로 복사되었습니다.\n",
            "fried_rice파일이 train_data에서 valid_data로 복사되었습니다.\n",
            "momos파일이 train_data에서 valid_data로 복사되었습니다.\n",
            "pizza파일이 train_data에서 valid_data로 복사되었습니다.\n",
            "idli파일이 train_data에서 valid_data로 복사되었습니다.\n",
            "Crispy Chicken파일이 train_data에서 valid_data로 복사되었습니다.\n",
            "samosa파일이 train_data에서 valid_data로 복사되었습니다.\n",
            "paani_puri파일이 train_data에서 valid_data로 복사되었습니다.\n",
            "kulfi파일이 train_data에서 valid_data로 복사되었습니다.\n",
            "pav_bhaji파일이 train_data에서 valid_data로 복사되었습니다.\n",
            "burger파일이 train_data에서 valid_data로 복사되었습니다.\n",
            "omelette파일이 train_data에서 valid_data로 복사되었습니다.\n",
            "Taco파일이 train_data에서 valid_data로 복사되었습니다.\n",
            "jalebi파일이 train_data에서 valid_data로 복사되었습니다.\n",
            "Fries파일이 train_data에서 valid_data로 복사되었습니다.\n",
            "dhokla파일이 train_data에서 valid_data로 복사되었습니다.\n",
            "dal_makhani파일이 train_data에서 valid_data로 복사되었습니다.\n",
            "Taquito파일이 train_data에서 valid_data로 복사되었습니다.\n",
            "Hot Dog파일이 train_data에서 valid_data로 복사되었습니다.\n",
            "Baked Potato파일이 train_data에서 valid_data로 복사되었습니다.\n",
            "butter_naan파일이 train_data에서 valid_data로 복사되었습니다.\n",
            "sushi파일이 train_data에서 valid_data로 복사되었습니다.\n",
            "pakode파일이 train_data에서 valid_data로 복사되었습니다.\n",
            "chicken_curry파일이 train_data에서 valid_data로 복사되었습니다.\n",
            "chapati파일이 train_data에서 valid_data로 복사되었습니다.\n",
            "masala_dosa파일이 train_data에서 valid_data로 복사되었습니다.\n",
            "kadai_paneer파일이 train_data에서 valid_data로 복사되었습니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### train에서 valid, test로 옮기고 난 뒤 해당 폴더에 제대로 move가 되었는지 확인"
      ],
      "metadata": {
        "id": "i57647SyUEVa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "food_list = os.listdir(trainPath)\n",
        "print(food_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYmIy6-gaY7p",
        "outputId": "e3d86c72-624c-4815-dee9-5121dbcd3875"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['kaathi_rolls', 'ice_cream', 'Sandwich', 'Donut', 'chole_bhature', 'chai', 'apple_pie', 'cheesecake', 'fried_rice', 'momos', 'pizza', 'idli', 'Crispy Chicken', 'samosa', 'paani_puri', 'kulfi', 'pav_bhaji', 'burger', 'omelette', 'Taco', 'jalebi', 'Fries', 'dhokla', 'dal_makhani', 'Taquito', 'Hot Dog', 'Baked Potato', 'butter_naan', 'sushi', 'pakode', 'chicken_curry', 'chapati', 'masala_dosa', 'kadai_paneer']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "check_train = os.listdir(trainPath)\n",
        "for Tr_list in check_train:\n",
        "    print(Tr_list, len(os.listdir(os.path.join(trainPath, Tr_list))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JbZK7fXUUAb3",
        "outputId": "54cf7b5e-bedd-41a1-c677-cef4a972ea88",
        "collapsed": true
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kaathi_rolls 168\n",
            "ice_cream 600\n",
            "Sandwich 900\n",
            "Donut 900\n",
            "chole_bhature 226\n",
            "chai 207\n",
            "apple_pie 600\n",
            "cheesecake 600\n",
            "fried_rice 210\n",
            "momos 195\n",
            "pizza 165\n",
            "idli 182\n",
            "Crispy Chicken 900\n",
            "samosa 147\n",
            "paani_puri 87\n",
            "kulfi 129\n",
            "pav_bhaji 183\n",
            "burger 199\n",
            "omelette 600\n",
            "Taco 900\n",
            "jalebi 174\n",
            "Fries 900\n",
            "dhokla 147\n",
            "dal_makhani 177\n",
            "Taquito 900\n",
            "Hot Dog 930\n",
            "Baked Potato 900\n",
            "butter_naan 185\n",
            "sushi 600\n",
            "pakode 162\n",
            "chicken_curry 600\n",
            "chapati 197\n",
            "masala_dosa 165\n",
            "kadai_paneer 201\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "check_valid = os.listdir(validPath)\n",
        "for V_list in check_valid:\n",
        "    print(V_list, len(os.listdir(os.path.join(validPath, V_list))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfZq9G3sUvNM",
        "outputId": "e43d992c-8a3a-4e57-abaf-cfbc6c1e43ca",
        "collapsed": true
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kaathi_rolls 56\n",
            "ice_cream 200\n",
            "Sandwich 300\n",
            "Donut 300\n",
            "chole_bhature 75\n",
            "chai 69\n",
            "apple_pie 200\n",
            "cheesecake 200\n",
            "fried_rice 70\n",
            "momos 64\n",
            "pizza 55\n",
            "idli 60\n",
            "Crispy Chicken 300\n",
            "samosa 49\n",
            "paani_puri 29\n",
            "kulfi 43\n",
            "pav_bhaji 61\n",
            "burger 66\n",
            "omelette 200\n",
            "Taco 300\n",
            "jalebi 57\n",
            "Fries 300\n",
            "dhokla 49\n",
            "dal_makhani 59\n",
            "Taquito 300\n",
            "Hot Dog 309\n",
            "Baked Potato 300\n",
            "butter_naan 61\n",
            "sushi 200\n",
            "pakode 54\n",
            "chicken_curry 200\n",
            "chapati 65\n",
            "masala_dosa 54\n",
            "kadai_paneer 67\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "check_test = os.listdir(testPath)\n",
        "for Te_list in check_test:\n",
        "    print(Te_list, len(os.listdir(os.path.join(testPath, Te_list))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKE0b1Kdf9uq",
        "outputId": "6a274aaf-9c75-4e7b-f62e-75c95445f4c6",
        "collapsed": true
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kaathi_rolls 55\n",
            "ice_cream 200\n",
            "Sandwich 300\n",
            "Donut 300\n",
            "chole_bhature 75\n",
            "chai 68\n",
            "apple_pie 200\n",
            "cheesecake 200\n",
            "fried_rice 70\n",
            "momos 64\n",
            "pizza 55\n",
            "idli 60\n",
            "Crispy Chicken 300\n",
            "samosa 48\n",
            "paani_puri 28\n",
            "kulfi 42\n",
            "pav_bhaji 61\n",
            "burger 66\n",
            "omelette 200\n",
            "Taco 300\n",
            "jalebi 57\n",
            "Fries 300\n",
            "dhokla 49\n",
            "dal_makhani 59\n",
            "Taquito 300\n",
            "Hot Dog 309\n",
            "Baked Potato 300\n",
            "butter_naan 61\n",
            "sushi 200\n",
            "pakode 53\n",
            "chicken_curry 200\n",
            "chapati 65\n",
            "masala_dosa 54\n",
            "kadai_paneer 66\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 데이터의 정규화를 위해 평균과 표준편차를 구하기"
      ],
      "metadata": {
        "id": "APKM9GfD5cwU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "def cal_mean_std(img_path):\n",
        "\n",
        "\n",
        "    mean = torch.zeros(3, dtype=torch.float)\n",
        "    std_sum = torch.zeros(3, dtype=torch.float)  # 분산을 누적하기 위한 변수\n",
        "    total_pixels = 0  # 전체 픽셀 수를 저장\n",
        "\n",
        "    if os.path.isdir(img_path):  # img_path가 폴더인지 확인\n",
        "        for folder_name in os.listdir(img_path):  # 각 폴더 이름을 가져옴\n",
        "            folderPath = os.path.join(img_path, folder_name)\n",
        "            if os.path.isdir(folderPath):  # 폴더인지 확인\n",
        "                for filename in os.listdir(folderPath):  # 폴더 내의 파일 목록 가져옴\n",
        "                    imgPath = os.path.join(folderPath, filename)\n",
        "                    if os.path.isfile(imgPath):\n",
        "                        img = Image.open(imgPath)\n",
        "                        # ToTensor()를 사용하여 0~1로 정규화\n",
        "                        tensor_image = transforms.ToTensor()(img)\n",
        "\n",
        "                        total_pixels += tensor_image.shape[1] * tensor_image.shape[2]\n",
        "\n",
        "                        # 채널별 픽셀 값을 모두 합산\n",
        "                        for i in range(3):\n",
        "                            mean[i] += tensor_image[i, :, :].sum()\n",
        "\n",
        "        # 전체 픽셀 수로 나눠서 평균 계산\n",
        "        mean /= total_pixels\n",
        "\n",
        "        # 전체 이미지 데이터셋의 표준 편차 계산\n",
        "\n",
        "        for folder_name in os.listdir(img_path):\n",
        "            folderPath = os.path.join(img_path, folder_name)\n",
        "            if os.path.isdir(folderPath):\n",
        "                    for filename in os.listdir(folderPath):  # 폴더 내의 파일 목록 가져옴\n",
        "                        imgPath = os.path.join(folderPath, filename)\n",
        "                        if os.path.isfile(imgPath):\n",
        "                            img = Image.open(imgPath)\n",
        "                            # ToTensor()를 사용하여 0~1로 정규화\n",
        "                            tensor_image = transforms.ToTensor()(img)\n",
        "                            for i in range(3):\n",
        "                                std_sum[i] += ((tensor_image[i, :, :] - mean[i]) ** 2).sum()  # 분산을 누적\n",
        "\n",
        "        std = torch.sqrt(std_sum / total_pixels)  # 전체 이미지 데이터셋의 표준 편차 계산\n",
        "\n",
        "        return mean, std\n",
        "    else:\n",
        "        print(f\"Error: '{img_path}' is not a directory.\")\n",
        "\n",
        "        return None, None"
      ],
      "metadata": {
        "id": "crb3stKm2d98"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### train, valid transforms 만들기"
      ],
      "metadata": {
        "id": "bny_IGQcbBHg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "org_size = (256, 256)\n",
        "img_size = (224, 224)\n",
        "\n",
        "brightness = 0.1\n",
        "contrast = 0.1\n",
        "saturation = 0.1\n",
        "hue = 0.1\n",
        "\n",
        "mean_train, std_train =  cal_mean_std(trainPath)\n",
        "mean_valid, std_valid =  cal_mean_std(validPath)\n",
        "mean_test, std_test =  cal_mean_std(testPath)\n",
        "\n",
        "\n",
        "# train data를 증식\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize(org_size),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(224),\n",
        "    transforms.ColorJitter(brightness=brightness, contrast=contrast, saturation=saturation, hue = hue),\n",
        "    transforms.ToTensor()\n",
        "    ,\n",
        "    transforms.Normalize(mean = mean_train, std = std_train)\n",
        "\n",
        "])\n",
        "valid_transform = transforms.Compose([\n",
        "    transforms.Resize(org_size),\n",
        "    transforms.ToTensor()\n",
        "    ,\n",
        "    transforms.Normalize(mean = mean_valid, std = std_valid)\n",
        "\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize(org_size),\n",
        "    transforms.ToTensor()\n",
        "    ,\n",
        "    transforms.Normalize(mean = mean_test, std = std_test)\n",
        "\n",
        "])"
      ],
      "metadata": {
        "id": "4cntaKKvbLfl"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### train, valid dataset, loader 만들기"
      ],
      "metadata": {
        "id": "nZXnBhLy52np"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "batchsize = 32\n",
        "\n",
        "train_dataset = datasets.ImageFolder(root=trainPath, transform=train_transform)\n",
        "valid_dataset = datasets.ImageFolder(root=validPath, transform=test_transform)\n",
        "test_dataset = datasets.ImageFolder(root=testPath, transform=test_transform)\n",
        "\n",
        "# 데이터 로더 생성 (batch_size는 필요에 따라 조정)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batchsize, shuffle=True)\n",
        "class_train_idx = train_dataset.class_to_idx\n",
        "print(class_train_idx)\n",
        "\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=batchsize, shuffle=False)\n",
        "class_valid_idx = valid_dataset.class_to_idx\n",
        "print(class_valid_idx)\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=batchsize, shuffle=False)\n",
        "class_test_idx = test_dataset.class_to_idx\n",
        "print(class_test_idx)"
      ],
      "metadata": {
        "id": "i7qiDy-xWQSc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d692d80-21b5-429f-d0cd-83e968c5541b"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Baked Potato': 0, 'Crispy Chicken': 1, 'Donut': 2, 'Fries': 3, 'Hot Dog': 4, 'Sandwich': 5, 'Taco': 6, 'Taquito': 7, 'apple_pie': 8, 'burger': 9, 'butter_naan': 10, 'chai': 11, 'chapati': 12, 'cheesecake': 13, 'chicken_curry': 14, 'chole_bhature': 15, 'dal_makhani': 16, 'dhokla': 17, 'fried_rice': 18, 'ice_cream': 19, 'idli': 20, 'jalebi': 21, 'kaathi_rolls': 22, 'kadai_paneer': 23, 'kulfi': 24, 'masala_dosa': 25, 'momos': 26, 'omelette': 27, 'paani_puri': 28, 'pakode': 29, 'pav_bhaji': 30, 'pizza': 31, 'samosa': 32, 'sushi': 33}\n",
            "{'Baked Potato': 0, 'Crispy Chicken': 1, 'Donut': 2, 'Fries': 3, 'Hot Dog': 4, 'Sandwich': 5, 'Taco': 6, 'Taquito': 7, 'apple_pie': 8, 'burger': 9, 'butter_naan': 10, 'chai': 11, 'chapati': 12, 'cheesecake': 13, 'chicken_curry': 14, 'chole_bhature': 15, 'dal_makhani': 16, 'dhokla': 17, 'fried_rice': 18, 'ice_cream': 19, 'idli': 20, 'jalebi': 21, 'kaathi_rolls': 22, 'kadai_paneer': 23, 'kulfi': 24, 'masala_dosa': 25, 'momos': 26, 'omelette': 27, 'paani_puri': 28, 'pakode': 29, 'pav_bhaji': 30, 'pizza': 31, 'samosa': 32, 'sushi': 33}\n",
            "{'Baked Potato': 0, 'Crispy Chicken': 1, 'Donut': 2, 'Fries': 3, 'Hot Dog': 4, 'Sandwich': 5, 'Taco': 6, 'Taquito': 7, 'apple_pie': 8, 'burger': 9, 'butter_naan': 10, 'chai': 11, 'chapati': 12, 'cheesecake': 13, 'chicken_curry': 14, 'chole_bhature': 15, 'dal_makhani': 16, 'dhokla': 17, 'fried_rice': 18, 'ice_cream': 19, 'idli': 20, 'jalebi': 21, 'kaathi_rolls': 22, 'kadai_paneer': 23, 'kulfi': 24, 'masala_dosa': 25, 'momos': 26, 'omelette': 27, 'paani_puri': 28, 'pakode': 29, 'pav_bhaji': 30, 'pizza': 31, 'samosa': 32, 'sushi': 33}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-ignite"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jvz1k4YDeaRu",
        "outputId": "7ab8c390-4285-4792-99ab-60a0a39ab938"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytorch-ignite in /usr/local/lib/python3.10/dist-packages (0.5.1)\n",
            "Requirement already satisfied: torch<3,>=1.3 in /usr/local/lib/python3.10/dist-packages (from pytorch-ignite) (2.4.1+cu121)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pytorch-ignite) (24.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.3->pytorch-ignite) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.3->pytorch-ignite) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.3->pytorch-ignite) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.3->pytorch-ignite) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.3->pytorch-ignite) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.3->pytorch-ignite) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3,>=1.3->pytorch-ignite) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<3,>=1.3->pytorch-ignite) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### early stopping 설정"
      ],
      "metadata": {
        "id": "E9O7D8n0iHdO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# EarlyStopping 클래스\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt'):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): 성능 개선이 없을 때 몇 번의 에포크까지 기다릴지.\n",
        "            verbose (bool): True일 경우 개선될 때마다 메시지 출력.\n",
        "            delta (float): 성능 개선으로 간주될 최소 변화량.\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = float('inf')\n",
        "        self.delta = delta\n",
        "        self.path = path\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "        score = val_loss\n",
        "        # 처음에 호출됐을때는 best_score가 None이라서 초기값을 설정\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        # 지금까지의 best_score와 현재 score를 비교\n",
        "        elif score > self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            if self.verbose:\n",
        "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "                print(f'val_loss :{score}')\n",
        "            # patience값이 모두 충족했을때, 종료조건이 만족될때\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''검증 손실이 감소하면 모델을 저장합니다.'''\n",
        "        if self.verbose:\n",
        "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "            torch.save(model.state_dict(), self.path)  # 모델 상태 저장\n",
        "        self.val_loss_min = val_loss\n",
        "\n",
        "# EarlyStopping 인스턴스 생성 (patience=10)\n",
        "early_stopping = EarlyStopping(patience=10, verbose=True, path='resnet50_best.pth')"
      ],
      "metadata": {
        "id": "eurqrHwWeaMz"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 모델 설정"
      ],
      "metadata": {
        "id": "neLvZOgKzSrA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.models as models\n",
        "from torchvision.models import ResNet50_Weights\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.models import resnet50, mobilenet_v3_large\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n"
      ],
      "metadata": {
        "id": "sImielS4mGmG"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ModifiedResNet50(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ModifiedResNet50, self).__init__()\n",
        "        self.base_model = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
        "        num_ftrs = self.base_model.fc.in_features\n",
        "        self.base_model.fc = nn.Linear(num_ftrs, 1024)\n",
        "        self.bn1 = nn.BatchNorm1d(1024)\n",
        "        self.fc1 = nn.Linear(1024, 512)\n",
        "        self.bn2 = nn.BatchNorm1d(512)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(512, 34)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.base_model(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "CYLMveD5mEN9"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 분류기 설정"
      ],
      "metadata": {
        "id": "QihS82EmiW26"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LR = 0.00001\n",
        "EPOCHS = 1\n",
        "model = ModifiedResNet50()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6PdPJWFm8Dq",
        "outputId": "95ff197a-3e2d-4e4c-9c21-d5c638c45d13"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ModifiedResNet50(\n",
              "  (base_model): ResNet(\n",
              "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu): ReLU(inplace=True)\n",
              "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "    (layer1): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (layer2): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (3): Bottleneck(\n",
              "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (layer3): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (3): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (4): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (5): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (layer4): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "    (fc): Linear(in_features=2048, out_features=1024, bias=True)\n",
              "  )\n",
              "  (bn1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (fc1): Linear(in_features=1024, out_features=512, bias=True)\n",
              "  (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              "  (fc2): Linear(in_features=512, out_features=34, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 모델 훈련"
      ],
      "metadata": {
        "id": "bKwM1t86iY3e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def fit(model, criterion, optimizer, epochs, train_loader, valid_loader, resume=False):\n",
        "\n",
        "    # 그래프로 출력하기 위한 리스트\n",
        "    if resume==False:\n",
        "        train_losses = []\n",
        "        train_accuracies = []\n",
        "        valid_losses = []\n",
        "        valid_accuracies = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        start = time.time()\n",
        "\n",
        "        model.train()\n",
        "\n",
        "        train_loss = 0\n",
        "        train_acc = 0\n",
        "        train_correct = 0\n",
        "\n",
        "        for train_x, train_y in tqdm(train_loader):\n",
        "            train_x, train_y = train_x.to(device), train_y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            pred = model(train_x)\n",
        "            loss = criterion(pred, train_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            _, prediction = torch.max(pred.cpu(), 1)\n",
        "            train_loss += loss.item()\n",
        "            train_correct += prediction.eq(train_y.cpu()).int().sum()\n",
        "\n",
        "        valid_loss = 0\n",
        "        valid_acc = 0\n",
        "        valid_correct = 0\n",
        "\n",
        "        for valid_x, valid_y in valid_loader:\n",
        "            with torch.no_grad():\n",
        "                model.eval()\n",
        "                valid_x, valid_y = valid_x.to(device), valid_y.to(device)\n",
        "                pred = model(valid_x)\n",
        "                loss = criterion(pred, valid_y)\n",
        "                _, prediction = torch.max(pred.cpu(), 1)\n",
        "                valid_loss += loss.item()\n",
        "                valid_correct += prediction.eq(valid_y.cpu()).int().sum()\n",
        "\n",
        "        train_acc = train_correct/len(train_loader.dataset)\n",
        "        valid_acc = valid_correct/len(valid_loader.dataset)\n",
        "\n",
        "        end = time.time()\n",
        "\n",
        "        print(f'{end- start:.3f}sec : [Epoch {epoch+1}/{epochs}] -> train loss: {train_loss/len(train_loader):.4f}, train acc: {train_acc*100:.3f}% / valid loss: {valid_loss/len(valid_loader):.4f}, valid acc: {valid_acc*100:.3f}%')\n",
        "\n",
        "        # 현재 LR값을 읽어올 수 있다\n",
        "        scheduler.step(valid_loss)\n",
        "        now_lr = optimizer.param_groups[0]['lr']\n",
        "        print(f'learning_rate {epoch+1}: {now_lr:.8f}')\n",
        "\n",
        "\n",
        "        # EarlyStopping을 호출하여 학습 중단 여부 확인\n",
        "        early_stopping(valid_loss, model)\n",
        "\n",
        "        # 학습 중단 조건을 충족하면 break\n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break\n",
        "        print('-' * 70)"
      ],
      "metadata": {
        "id": "wuaQ9Cd5iWkv"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습\n",
        "fit(model, criterion, optimizer, EPOCHS, train_loader, valid_loader, resume=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSJAqCqBvdPs",
        "outputId": "dbf26eca-f398-4a45-a99e-da254fae6eaf"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 448/448 [04:59<00:00,  1.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "357.741sec : [Epoch 1/1] -> train loss: 2.4303, train acc: 37.695% / valid loss: 1.4711, valid acc: 68.357%\n",
            "learning_rate 1: 0.00001000\n",
            "Validation loss decreased (inf --> 220.663999).  Saving model ...\n",
            "----------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 예측이 다른 이미지 불러오기"
      ],
      "metadata": {
        "id": "mFuWL3S3Fbi7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "# 테스트 데이터에서 성능을 평가하고, 예측이 잘못된 이미지를 시각화하는 함수\n",
        "def evaluate_and_visualize(model, test_loader, criterion):\n",
        "    test_loss = 0\n",
        "    test_correct = 0\n",
        "    test_total = 0\n",
        "\n",
        "    incorrect_labels = []\n",
        "    incorrect_preds = []\n",
        "    incorrect_filenames = []\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for test_x, test_y, filenames in tqdm(test_loader):\n",
        "            test_x, test_y = test_x.to(device), test_y.to(device)\n",
        "            pred = model(test_x)\n",
        "            loss = criterion(pred, test_y.cpu())\n",
        "            test_loss += loss.item()\n",
        "            _, prediction = torch.max(pred.cpu(), 1)\n",
        "\n",
        "            # 정확도 계산\n",
        "            test_correct += prediction.eq(test_y.cpu()).int().sum()\n",
        "            test_total += test_y.size(0)\n",
        "\n",
        "            if prediction != test_y:\n",
        "                incorrect_labels.append(test_y.cpu().numpy())\n",
        "                incorrect_preds.append(prediction.cpu().numpy())\n",
        "                incorrect_filenames.append(filenames[0])\n",
        "\n",
        "    test_accuracy = test_correct / test_total\n",
        "    print(\"test_correct:{}, test_total:{}\".format(test_correct, test_total))\n",
        "    print(f'Test Loss: {test_loss / len(test_loader):.4f}, Test Accuracy: {test_accuracy * 100:.2f}%')\n",
        "\n",
        "    print(len(incorrect_filenames))\n",
        "\n",
        "    # 예측이 잘못된 이미지 시각화\n",
        "    fig = plt.figure(figsize=(12, 12))\n",
        "    for i in range(min(16, len(incorrect_filenames))):\n",
        "        ax = fig.add_subplot(4, 4, i + 1)\n",
        "        image = Image.open(incorrect_filenames[i])\n",
        "        ax.imshow(image)\n",
        "        true_label = class_names[incorrect_labels[i][0]]  # 클래스 이름 사용\n",
        "        pred_label = class_names[incorrect_preds[i][0]]  # 클래스 이름 사용\n",
        "        filename = os.path.basename(incorrect_filenames[i])\n",
        "        ax.set_title(f'Pred: {pred_label}, Label: {true_label}, {filename}', fontsize=10)\n",
        "        ax.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "jL7T8xBkFbP2"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classnames = class_test_idx\n",
        "class_names = list(classnames.keys())\n",
        "evaluate_and_visualize(model, test_loader, criterion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "k9SHSVIvPPaO",
        "outputId": "25f682f1-65fe-4db1-993b-cc20acc08249"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/149 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "not enough values to unpack (expected 3, got 2)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-010ea0bdbc9d>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mclassnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclass_test_idx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mclass_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassnames\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mevaluate_and_visualize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-57-314fe1869d87>\u001b[0m in \u001b[0;36mevaluate_and_visualize\u001b[0;34m(model, test_loader, criterion)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilenames\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XdHlSMSKUfuv"
      }
    }
  ]
}